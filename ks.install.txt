# 19.08.2024
# Testen
  helm template -f global-values.yaml zalando-pg
# Installieren
  helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
  # helm install --debug --dry-run -n folio-backend -f global-values.yaml zal-pg ./zalando-pg  -- ähnlich wie helm template, macht nichts, gibt das YAML aus
  kubectl describe -n folio-backend pods
# Löschen
  # helm delete -n folio-backend zal-pg
  helm uninstall -n folio-backend zal-pg

  Ein Secret liegt in einer privaten Container-Registry
  Anleitung: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  docker login
  A Kubernetes cluster uses the Secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.
  If you already ran docker login, you can copy that credential into Kubernetes:
  kubectl create secret generic regcred --from-file=.dockerconfigjson=$HOME/.docker/config.json --type=kubernetes.io/dockerconfigjson
    secret/regcred created
  To understand the contents of the regcred Secret you created, start by viewing the Secret in YAML format:
    kubectl get secret regcred --output=yaml
  To understand what is in the .dockerconfigjson field, convert the secret data to a readable format:
    kubectl get secret regcred --output="jsonpath={.data.\.dockerconfigjson}" | base64 --decode
  Das gleiche Secret habe ich auch noch einmal mit anderem Namen im Namensraum "folio-backend" erzeugt:
    kubectl get secret -n folio-backend folio-hbz-dockerhub-key --output="jsonpath={.data.\.dockerconfigjson}" | base64 --decode

  To pull the image from the private registry, Kubernetes needs credentials. 
  The imagePullSecrets field in the configuration file specifies that Kubernetes should get the credentials from a Secret named gitlab-folio-group .
  The image is: gitlab.bib-bvb.de:5050/folio/folio-tools:debian-v1.3
  In Job folio.init.job.yaml
  In turn, the image itself need environment variables from a secret key ref:
         env:
        - name: PGPASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres.folio-release-name.credentials.postgresql.acid.zalan.do
              key: password
   Weiterlesen hier https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/  "What's next"

  Für den Moment einfach nur postgresql auf das Cluster bringen, ohne die Secrets, User- und Init-Jobs/Scripts; so:
    kubectl -n folio-backend -f folio-zal-pg.yml apply
      Warning ...
      postgresql.acid.zalan.do/folio-zal-pg configured

22.08.2024
  https://kubernetes-io.translate.goog/docs/concepts/containers/images/?_x_tr_sl=auto&_x_tr_tl=de&_x_tr_hl=de#using-a-private-registry
    Lesen: Angeben von ImagePullSecrets auf einem Pod  (bekannt)
  aus folio-public/folio-tools ein Image bauen  ==> Das brauche ich nur, wenn ich nicht von bvb/folio-public herunter ziehen kann
    docker build https://gitlab.bib-bvb.de/folio-public/folio-tools
       Error response from daemon: dockerfile parse error line 5: unknown instruction: <!DOCTYPE
   anders: Repo klonen nach ~/folio-tools, dann
     cd ~/folio-tools
     docker build .
      Sending build context to Docker daemon  70.66kB
      Step 1/6 : FROM debian:bookworm-slim
      Get "https://registry-1.docker.io/v2/": dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:48605->[::1]:53: read: connection refused
---------------
   Erneuter Versuch zal-pg Deployment, nachdem Florian K. das images in der values.yml auf "folio-public" geändert hat
      helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
         error: INSTALLATION FAILED: failed post-install: timed out waiting for the condition
   create-pgsuperuser schlägt fehl (FAIL), Log ist dieses (das wird immer wieder versucht):
   - Waiting for file kube-scripts/create-pgsuperuser.sh...
    testing if kube-scripts/create-pgsuperuser.sh exists...done
    Thu Aug 22 17:43:12 UTC 2024 Creating SUPERUSER pgsuperuser ...
    psql: error: connection to server at "folio-zal-pg" (10.43.166.10), port 5432 failed: Connection refused
    ********************************************************************************************************
     Is the server running on that host and accepting TCP/IP connections?
         kubectl describe -n folio-backend pods
    Das Herunterziehen von folio-public geht jetzt !
   Beim Job "create-pgsuperuser" kommen jetzt folgende Events:
Events:
  Type    Reason     Age        From                  Message
  ----    ------     ----       ----                  -------
  Normal  Scheduled  <unknown>                        Successfully assigned folio-backend/create-pgsuperuser-snk6t to folio-k8s13
  Normal  Pulled     6m34s      kubelet, folio-k8s13  Container image "gitlab.bib-bvb.de:5050/folio-public/folio-tools:debian-v1.3" already present on machine
  Normal  Created    6m34s      kubelet, folio-k8s13  Created container create-pg-superuser
  Normal  Started    6m34s      kubelet, folio-k8s13  Started container create-pg-superuser
     Das passiert 6x (warum ?)
    - das Secret postgres.folio-zal-pg.credentials.postgresql.acid.zalan.do mit dem Passwort für den db-user postgres wurde angelegt.

      - "folio-zal-pg" steht auf Status: Pending und Grund: 0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.
                          - der Pod folio-zal-pg hat VolumeMounts und Volumes, aber sind die richtig definiert ?
                            die haben weder "name" noch "claimName"
      - aber auch das PVC "pgdata-folio-zal-pg" steht auf Status: Pending.
                -- der PVC sieht richtig aus, aber irgendwie gibt es kein Volume und keinen VolumeMount dazu. Siehe Cluster -> Persistent Volumes
                        die StorageClass local-path hat übrigens den Provisioner rancher.io
        das (folio-zal-pg und pgdate-folio-zal-pg) sind beides Services und ihre Manifestationen (YAML) sind im Dashboard unter Service - Services einsehbar.

23.08.
  in folio-zal-pg Pid
       volumeMounts:
        - mountPath: /home/postgres/pgdata
          name: pgdata
	  volumes:
    - name: pgdata
      persistentVolumeClaim:
        claimName: pgdata-folio-zal-pg-0
    - volumeName: pvc*   fehlt

    kind: postgres - Custom Resource Definition
     kubectl get crd

     nach Anpassen der CPUs / RAM, hat Pod folio-zal-pg jetzt den Status "Running" !
     Im Log kommen aber noch Fehler:
     Success. You can now start the database server using:
    /usr/lib/postgresql/13/bin/pg_ctl -D /home/postgres/pgdata/pgroot/data -l logfile start
2024-08-23 12:48:57.610 GMT [208] LOG:  invalid value for parameter "lc_monetary": "de_DE.UTF-8"
2024-08-23 12:48:57.610 GMT [208] LOG:  invalid value for parameter "lc_numeric": "de_DE.UTF-8"
2024-08-23 12:48:57.610 GMT [208] LOG:  invalid value for parameter "lc_time": "de_DE.UTF-8"
2024-08-23 14:48:57.610 CEST [208] FATAL:  configuration file "/home/postgres/pgdata/pgroot/data/postgresql.conf" contains er

      Nützliche Befehle:
       helm -n folio-backend list   - eine Liste, was in dem Namespace alles deployed ist.
       helm list - dasselbe für alle Namespaces
      mal das Log vom postgres-operator verfolgen:
       kubectl --namespace=default logs  "postgres-operator-8bc78d9f8-tzhhm"
      die verwendete Manifestation des Helm-Charts für zalando-pg, angereicherte Version:
       kubectl get -o yaml postgresqls.acid.zalan.do
       kubectl get postgresqls.acid.zalan.do   - Basisinformationen; was läuft
       kubectl get crd - listet Custom Resource Definitionen auf ("postgresql" von Zalando ist z.B. eine solche!)


      Florian: Jetzt noch einmal starten:
 helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
    - bei Job create-pgsuperuser kommt wieder "connection refused"
    Error: INSTALLATION FAILED: failed post-install: 1 error occurred:
        * job create-pgsuperuser failed: BackoffLimitExceeded
   -> evtl. noch einmal alles löschen und die "lc_monetary" ... Fehler in postgresql.conf beheben

26.08.2024
  UTF-8 => utf8 in den Parametern lc_monetary etc. (in values.yaml)
  -- Waiting for file kube-scripts/create-pgsuperuser.sh...
  testing if kube-scripts/create-pgsuperuser.sh exists...done
  Mon Aug 26 10:57:52 UTC 2024 Creating SUPERUSER pgsuperuser ...
  psql: error: connection to server at "folio-zal-pg" (10.43.21.168), port 5432 failed: Connection refused
   Is the server running on that host and accepting TCP/IP connections?

   Jetzt die Parameter entfernt (lc_monetary, ...)
   - in Datenbanklog kommen CREATE und EXEC Anweisungen (gut)
  2024-08-26 11:10:34,381 WARNING: Could not activate Linux watchdog device: Can't open watchdog device: [Errno 2] No such file or directory: '/dev/watchdog'
  - der Job create-pgsuperuser schlägt weiterhin fehl (Verbindung zur Datenbank kann nicht hergestellt werden)
  - das persistente Volume für die pg-Datenbank wurde definitiv erfolgreich zur Verfügung gestellt. Die Datenbank wurde hier angelegt:
    root@folio-k8s13:/var/lib/rancher/k3s/storage/pvc-d016e739-d9db-4634-9182-13b98c9eb19c_folio-backend_pgdata-folio-zal-pg-0/pgroot/
  Jetzt kommen Warnungen im Postgres Log: 
  2024-08-26 11:10:36,057 WARNING: Kubernetes RBAC doesn't allow GET access to the 'kubernetes' endpoint in the 'default' namespace. Disabling 'bypass_api_service'.
  2024-08-26 11:10:36,070 WARNING: postgresql parameter listen_addresses=* failed validation, defaulting to None
  2024-08-26 11:10:36,071 WARNING: postgresql parameter wal_keep_segments=0 failed validation, defaulting to 8
  2024-08-26 11:10:36,071 WARNING: postgresql parameter wal_log_hints=off failed validation, defaulting to on

  "uninstall" anstatt "delete" verwenden:
    helm uninstall -n folio-backend zal-pg
    release "zal-pg" uninstalled
Job zal-pg-folio-init-job geht jetzt:
  -- Waiting for file kube-scripts/folio-db-init.sh...
testing if kube-scripts/folio-db-init.sh exists...done
Mon Aug 26 11:44:55 UTC 2024 Creating databases okapi and folio ...
Mon Aug 26 11:44:55 UTC 2024 Running: psql -w -v ON_ERROR_STOP=1 -h folio-zal-pg -p 5432 -U postgres ...
CREATE ROLE
CREATE DATABASE
CREATE ROLE
CREATE DATABASE
Mon Aug 26 11:44:55 UTC 2024 Init script is completed

Job create-pgsuperuser war jetzt auch erfolgreich:
-- Waiting for file kube-scripts/create-pgsuperuser.sh...
testing if kube-scripts/create-pgsuperuser.sh exists...done
Mon Aug 26 11:44:52 UTC 2024 Creating SUPERUSER pgsuperuser ...
CREATE ROLE
Mon Aug 26 11:44:52 UTC 2024 User created!

Jetzt war erfolgreich:
helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
NAME: zal-pg
LAST DEPLOYED: Mon Aug 26 13:44:51 2024
NAMESPACE: folio-backend
STATUS: deployed
REVISION: 1
TEST SUITE: None

Jetzt läuft erfolgeich: Pods folio-zal-pg-0, folio-zal-pg-1 im Namespace folio-backend; mit den 4 Warnungen wie oben genannt.
Weiter mit Installation Okapi.

30.08.2024
Versuche, Okapi zu installieren
Versuche, offizielles Image von folio-org zu ziehen, dieses hier: https://hub.docker.com/r/folioci/okapi
  cd ~/folio-helm-hbz
  helm install -n folio-backend -f global-values.yaml okapi ./okapi
  helm uninstall -n folio-backend okapi
 Normal   Pulled     34s                kubelet            Successfully pulled image "folioorg/okapi:latest" in 815ms (815ms including waiting). Image size: 100319551 bytes.
  Warning  BackOff    6s (x6 over 79s)   kubelet            Back-off restarting failed container okapi in pod okapi-5f79959657-z6tl2_folio-backend(d0e8fd4c-cf0d-49d6-85e7-5ecc7b3d9005)
  Florian: Besser  Helm-Chart von IndexData verwenden.
 Uwe 
   podman pull gitlab.bib-bvb.de:5050/folio-public/okapi:v5.2.0
   Trying to pull gitlab.bib-bvb.de:5050/folio-public/okapi:v5.2.0...
   Error: initializing source docker://gitlab.bib-bvb.de:5050/folio-public/okapi:v5.2.0: Requesting bearer token: invalid status code from registry 403 (Forbidden)
   
Florian
https://github.com/indexdata/okapi-helm/tree/main
  To use this helm chart, first add the repository locally.
  helm repo add okapi https://indexdata.github.io/okapi-helm/
  Den Ingress erstmal auf false stellen.
  Okapi muss aber von außen erreichbar sein. Damit kommen die Überlegungen wie https, Ingress, Zertifikat ...
  Florian: Let's encrypt; sectigo
   Florian: kubectl Port forward; ein Kubernetes Service für Okapi
      https://kubernetes.io/docs/reference/kubectl/generated/kubectl_port-forward/#examples

09.09.2024
  cd ~/folio-helm-hbz
  git clone https://github.com/indexdata/okapi-helm.git
  cd okapi-helm
  edit okapi/values.yaml
  helm uninstall -n folio-backend okapi
  helm install -n folio-backend -f ../global-values.yaml okapi ./okapi

  1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace folio-backend -l "app.kubernetes.io/name=okapi,app.kubernetes.io/instance=okapi" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace folio-backend $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo $POD_NAME
okapi-6bc7d758cd-hvtrw
  echo $CONTAINER_PORT
9130
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace folio-backend port-forward $POD_NAME 8080:$CONTAINER_PORT
Forwarding from 127.0.0.1:8080 -> 9130
Forwarding from [::1]:8080 -> 9130

  kubectl describe -n folio-backend pods
  ...
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  6m9s                  default-scheduler  Successfully assigned folio-backend/okapi-6bc7d758cd-hvtrw to folio-k8s12
  Normal   Pulled     6m6s                  kubelet            Successfully pulled image "folioorg/okapi:5.1.2" in 3.652s (3.652s including waiting). Image size: 104746777 bytes.
  Normal   Pulled     5m56s                 kubelet            Successfully pulled image "folioorg/okapi:5.1.2" in 805ms (805ms including waiting). Image size: 104746777 bytes.
  Normal   Pulled     5m26s                 kubelet            Successfully pulled image "folioorg/okapi:5.1.2" in 807ms (807ms including waiting). Image size: 104746777 bytes.
  Normal   Created    4m56s (x4 over 6m6s)  kubelet            Created container okapi
  Normal   Started    4m56s (x4 over 6m6s)  kubelet            Started container okapi
  Normal   Pulled     4m56s                 kubelet            Successfully pulled image "folioorg/okapi:5.1.2" in 911ms (911ms including waiting). Image size: 104746777 bytes.
  Normal   Pulling    4m1s (x5 over 6m10s)  kubelet            Pulling image "folioorg/okapi:5.1.2"
  Warning  BackOff    67s (x22 over 5m48s)  kubelet            Back-off restarting failed container okapi in pod okapi-6bc7d758cd-hvtrw_folio-backend(be154412-cf25-4215-82ca-dab6fec04890)

  und im Log vom Pod okapi auf folio-k8s12:
2024-09-09T13:52:47,357 ERROR ? FATAL: password authentication failed for user "okapi" (28P01) io.vertx.pgclient.PgException: FATAL: password authentication failed for user "okapi" (28P01) | 
2024-09-09T13:52:47,358 INFO ? shutdown 

  In Dashboard - folio-backend - Services - okapi
Endpoints 
Host 		Ports (Name, Port, Protokoll) 		Node 		Bereit
10.42.1.39 	okapi,9130,TCP hazelcast,5701,TCP 	folio-k8s12
-
  Versuche, Okapi anzusprechen:
  curl -XGET http://folio-k8s12.hbz-nrw.de:9130/_/env
curl: (7) Failed to connect to folio-k8s12.hbz-nrw.de port 9130 after 1 ms: Couldn't connect to server
  Das kann aber an "password authentication failure" liegen.

Einen neuen Job und Skript create-pg-okapiuser eingerichtet. => nein, das ist alles schon im Job folio.db.init  geschehen.
Ich muss nur das richtige Okapi-Passwort verwenden.
Das Okapi-Passwort steht z.Zt. in values.yaml in folio-helm-hbz/okapi-helm/okapi secret drin.

Es kommt aber weiterhin
  2024-09-09T16:14:33,188 ERROR ? FATAL: password authentication failed for user "okapi" (28P01) io.vertx.pgclient.PgException: FATAL: password authentication failed for user "okapi" (28P01) | 

Wie wird das Passwort in dem IndexData Helm Chart gezogen, wie verbindet sich Okapi mit der Datenbank ?
- es gibt eine secret.yaml, die baut aus Values.secret eine okapi-config.json
- in deployment.yaml wird dann aus dem Secret ein Volume gebaut und dieses im Okapi-Container gemountet.
  => die Okapi-config.json anders bauen, nicht aus Values.secret sondern aus dem schon vorhandenen Secret folio-db-secret.
     Dann die anderen Parameter zu der okapi-config.json hinzufügen; nur das $OKAPI_PASSWORD ist geheim.
     secret.yaml baut einfach ein Secret "okapi" (=okapi.fullname) mit dem Inhalt StringData (=der Inhalt der okapi-config.json).

11.09.2024
  Quelle: https://stackoverflow.com/questions/59082646/use-kubernetes-secrets-as-environment-variables-inside-a-config-map
  die config map ist statisch, ich kann in sie keine Platzhalter herein schreiben
  Ich muss einen Job anlegen, der ein sh-Skript ausführt, welches die config-map schreibt.

   Achtung! YAMLs in templates.HIST/ werden auch ausgeführt, diese löschen !!
   Secret folio-db-secret geändert, dieses muss explizit gelöscht werden; das über das Dashboard machen.
   Das Secret wird nicht wieder angelegt, in der Folge wird folio.init.job nicht fertig :-(
     dann lege ich das Secret eben manuell wieder an: kubectl -n folio-backend -f /tmp/folio-db-secret.yaml apply
   Jetzt ist Okapi erfolgreich hoch gekommen:
  Normal  Started    2m    kubelet            Started container okapi
  Im log vom Pod Okapi:
  2024-09-11T13:30:54,990 INFO ? 592314/proxy REQ 10.42.1.1:46856 supertenant GET /_/proxy/health okapi-5.1.2 
  2024-09-11T13:30:54,990 INFO ? 913644/proxy RES 200 1047us okapi-5.1.2 /_/proxy/health 

   Als nächstes das mit dem Ingress ausprobieren;
   z.Zt. ist ingress: enabled: false !
   - wie geht das mit dem kubectl port-forward ?

13.09. 
Florian G.: Ich brauche eine Ingress - Klasse
  ich brauche Ingress-Controller, ein laufender nginx-Pod, der guckt, gibt es Ingress-Objekte von meiner Klasse.
   der Proxy-Server von außen muss auf die IP-Adressen von Kubernetes zugreifen;
      mit Ingress wird es auf k8s-Seite einfacher; Ingress-Controller installieren. Load-Balancer installieren
             (für Sachen von außen)
	     https://kubernetes.github.io/ingress-nginx
   in dem Netz, in dem die virtuellen Maschinen sind, muss ich mehrere IP-Adressen reserviert bekommen.
     Diese kann dann der Load-Balancer genutzen. Dann habe ich eine Load-Balancer IP-Adresse.
     Auf diese IP-Adresse setzt dann der Ingress-Controller auf.
       IP-Adresse kann sich ändern, Service-Name nicht.
       Der interen DNS-Server von K8s wird auch über IP-Adresse angepsrochen.

       Denis: den internern Diensten einen node-Port verpassen.
       Okapi an 3 Nodes...

16.09.
  Installation des nginx-Ingress-Controller
  helm upgrade --install ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx --namespace folio-backend
  kubectl get service ingress-nginx-controller --namespace=folio-backend
  NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP                     PORT(S)                      AGE
  ingress-nginx-controller   LoadBalancer   10.43.213.243   10.9.2.91,10.9.2.92,10.9.2.93   80:32702/TCP,443:31664/TCP   3m1s

19.09.2024
  hbz gitlab folio Projekt klonen:
  cd folio/folio-helm-hbz
  git clone http://quaoar4.hbz-nrw.de:8929/bms/folio.git folio-hbz
  # Geheime Informationen in quaoar-Repo speichern und nur symbolisch verknüpfen:
  ln -s folio-hbz/global-values.yaml
  ln -s folio-hbz/folio-hbz-dockerhub-key.yml

20.09.2024 d-sys-ops
  Okapi mit Service Type "NodePort" installieren
     cd okapi-helm  (Indexdata Helm Chart)
     vim okapi/values.yaml
       chaned one line:
        service:
   enabled: true
   name: okapi
-  type: ClusterIP
+  type: NodePort

     vim okapi/templates/service.yaml
       removed three lines:
        spec:
-  clusterIP: None
   type: {{ .Values.service.type }}
   ports:
     - port: {{ .Values.service.port }}
       name: okapi
-      nodePort: 9130
     - port: {{ .Values.service.second_port.port }}
       name: hazelcast
   selector:
     {{- include "okapi.selectorLabels" . | nindent 4 }}
-  type: NodePort

     helm upgrade -n folio-backend  okapi ./okapi
     kubectl get services -n folio-backend
     NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP                     PORT(S)
     ...
okapi                                NodePort       10.43.126.141   <none>                          9130:30935/TCP,5701:32127/TCP   148m
    jetzt geht: curl -XGET folio-k8s11:30935/_/env, mit einem beliebigen Servernamen der Server im Cluster
                                                    (auch einem Server, auf dem Okapi gar nicht läuft!)
    Der Port 30935 bleibt angeblich gleich, zumindest solange man Okapi nur mit "helm upgrade" verändert.

  wg. okapi-config.json, Datenbank-Passwort für den Okapi-Benutzer. Aus values-Datei vs. aus Secret:
    Florian nutzt keine okapi-config.json, sondern übergibt alle Okapi-Variablen als ENV-Vars !
    Dann kann er das Secret folio-db-secret auch für Okapi benutzen.
    Wie das geht: siehe meine Screenshots von heute:
      cd okapi/templates
      vim deployment.yaml:
      env
      -  name: PG_PASSWORD
         valueFrom:
	   secretKeyRef:
	     name: {{ .Values.global.db.folioSecret }} ==> folio-db-secret
	     key: {{ .Values.global.db.okapiPwKKey }}  ==> OKAPI_PASSOWRD
	deployment.yaml entsprechend angepasst.
    Das Secret okapi-config benötigt er nicht (davon ab, dass er sowieso kein Indexdata Helm Chart benutzt).
      => das in zalando_pg/templates/folio.secret.yaml wieder herausgeworfen.
    Und auch das Array in der .Values.secret wird nicht mehr benötigt; bei mir wurde das allerdings auch schon
    herausgeworfen und durch einen Eintrag in der folio-hbz/global-values.yaml ersetzt: dort auch herauswerfen. => OK, okapiPassword: okapi25 dort heraus geworfen.
    Alle anderen Parameter außer OKAPI_PASSWORD versuche ich weiterhin über eine okapi.conf zu laden => das Array .Values.secret wieder aktiviert, allerdings ohne postrges_password.
     cd okapi-helm
     helm upgrade -n folio-backend  -f ../global-values.yaml okapi ./okapi
     Release "okapi" has been upgraded. Happy Helming!
      NAME: okapi
      LAST DEPLOYED: Fri Sep 20 21:39:42 2024
      NAMESPACE: folio-backend
      STATUS: deployed
      REVISION: 6
    ok, hat funktioniert. Den okapi Ingress noch wieder entfernen (in values.yaml auf false setzen).
    curl -XGET folio-k8s11:30935/_/env []
    curl -XGET folio-k8s11:30935/_/proxy/health
      [ ]

    als nächstes Kafka und Elasticsearch als bitnami Helm Chart installieren, s. BVB-Repo / erl.

  nützliche Tipps / git:
  alle git-Commits zu einer bestimmten Datei anzeigen:
    git log -- values.yamlgit log -- values.yaml
  eine bestimmte git-Version einer Datei anzeigen: 
    wgit show dd2f3d9f124700a0a0149d00f43bb6a18f5c1773 -- values.yaml

  psql in einem pg-Container aufrufen; Dazu kann man sich einfach wie bei docker in den Container einloggen:
    kubectl -n folio-backend exec -it folio-zal-pg-0 -- bash
   oder nicht-interaktiv z.B. so:
    kubectl -n folio-backend exec folio-zal-pg-0 -- nslookup okapi
   In dem Container sind dann die Umgebungsvariablen bekannt, z.B. OKAPI_URL, SUPERUSER_PASSWD

   [ Diksussion über den Kubernetes Operator;
     - Wissen im Operator SDK ist nur während der Entwicklungsarbeiten notwendig, bei der Anwendung des Operators nicht mehr; https://sdk.operatorframework.io/build/
     - Kubernetes Operator wird die Module auch deployen
     - Upgrades mitdenken
     - Migration ist out-of-scope, da DB-Deployment auch nicht dazu gehört
   ]
   Ghassen: End-of-Live von Okapi ? 
     - zunächst ist es im Kubernetes Operator noch drin
     - auch mgr-application hat ein Feld OKAPI_URL (wird es genutzt ?)
     - Okapi-Support wird nicht vor Beginn des U-Releases enden.

   Ghassen: wer deployet die Module im Eureka-Context ?
   Florian: folio-org/mgr-applications ist dafür angedacht, aber es steht dort "optional". EBSCO macht es nicht damit.

    eureka-platform-boot : Eureka mit docker-compose klonen und das Skripgt start-docker-containers.sh ausprobieren,
      dann an den Fehlermeldungen weiter hangeln.
      git clone https://github.com/folio-org/eureka-platform-bootstrap.git
      Julian: Diese Plattform (Eureka ohne Kubernetes) wurde entwickelt, weil die AWS-Kosten für das Hosting
         (der Development-Plattform(en)) "unter die Decke gingen".

23.09.2024
  Kafka im Namensraum folio-staging-v1-kafka erneut installiert; mit Bitnami Helm Chart;
  Values-Datei kafka-hbz-staging-values.yml angepasst,
     siehe alles in  https://github.com/inkuss/kube.git .
  Installation Elasticsearch ebenfalls in diesem Repo beschrieben.
   
  weiter mit: Create a new tenant; dann: Install a FOLIO backend.

30.09.2024
  Einen Mandanten (diku) installieren:
  helm template tenant ./tenant > /tmp/hik2
  helm -n folio-backend install tenant ./tenant
  helm -n folio-backend uninstall  tenant

container "tenant-register-diku" in pod "tenant-register-diku-sj6d4" is waiting to start: CreateContainerConfigError
tenant-register-diku-sj6d4.17fa0d309880ad4f Failed Error: secret "supertenant-secret" not found

  -> Benötigt wird  env:
        - name: SUPERUSER_PASSWORD
          valueFrom:
            secretKeyRef:
              name: supertenant-secret
              key: password
  -> in files/regsiter-tenant.sh benötigt, um ein Superuser-Token zu holen.
    bei "bootstrap-superuser.pl" muss man das Token mitgeben: "If supertenant is secure, use the "st_token" argument to pass a token for the supertenant superuser."

   Das mal ohne Secure Superuser gemacht (auskommentiert)
    Creating tenant ... ()
     läuft auf Error
       Pod register-diku, Skript register-tenant.sh

    Fehler beim Deaktivieren von authtoken für tenant diku: Modul authtoken nicht gefunden.

    Error: INSTALLATION FAILED: failed post-install: 1 error occurred:
        * timed out waiting for the condition

   Das ist aber schon beim Schritt "Create a superuser". Der kommt erst, nachdem das Backend deployed wurde!

01.10.2024
  # Module installieren
  # Helm chart folio-helm-hbz/modules - das muss man zuerst machen. Erst dann Neuanlage eines Mandanten.
  # supertenant-secret wird dort auch angelegt
  helm install -n folio-backend mods-auth modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-auth.yaml
  NAME: mods-auth
  LAST DEPLOYED: Tue Oct  1 14:08:55 2024
  NAMESPACE: default
  STATUS: deployed
  REVISION: 1
  TEST SUITE: None
      das deployed mod-authtoken, mod-permissions, mod-login, mod-users
    helm -n folio-backend uninstall  mods-auth

  cd  ~/folio/folio-helm-hbz
  helm upgrade -n folio-backend mods-auth modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-auth.yaml
    - das löscht die Jobs nicht ab

  helm template mods-auth modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-auth.yaml > /tmp/hik2
    folio-tools:v1.1 -> v1.3

    Job init-mod-authtoken:
      /usr/src/app/docker-entrypoint.sh: line 14: /usr/src/app/kube-scripts/init-module.sh: cannot execute: required file not found

8.10.2024
  FOLIO Helm Charts bei https://github.com/folio-org/folio-helm-v2/tree/master/charts 
  helm repo add folio-helm-v2 https://repository.folio.org/repository/folio-helm-v2/
      "folio-helm-v2" has been added to your repositories
  helm install -n folio-backend mod-authtoken folio-helm-v2/mod-authtoken --version 2.14.24 -f global-values.yaml
  helm install -n folio-backend mod-login folio-helm-v2/mod-login -f global-values.yaml
  helm install -n folio-backend mod-users folio-helm-v2/mod-users -f global-values.yaml
  helm install -n folio-backend mod-permissions folio-helm-v2/mod-permissions -f global-values.yaml
  Die Module sind installiert ("running"), sie können aber nicht richtig konfiguriert sein.
  Z.B. ist die Anbindung an die Datenbank bei mod-users so konfiguriert (s. values.yml):
integrations:
  db:
    enabled: true
    host: postgresql               => folio-zal-pg
    hostReader: ""
    port: 5432
    database: folio
    username: admin                => folio (in folio-helm-hbz/zalando-pg/templates/scripts.yaml festgelegt)
    password: password             => FOLIO_PASSWORD aus folio-db-secret
  Es muss aber so sein (s. global-values.yaml):
  # Globale Variablen
global:
  db:
    size: "5Gi"
    storageClass: local-path
    maxpoolsize: 10
    host: folio-zal-pg
    port: 5432
    folioSecret: folio-db-secret      => FOLIO_PASSWORD, OKAPI_PASSWORD
    adminSecret: pg-superuser-secret  => PGSU_PASSWORD
    okapiPwKey: OKAPI_PASSWORD
    # okapiPassword: okapi25
    folioPwKey: FOLIO_PASSWORD
    allModuleAccess: true
    initDb: true

evtl. muss erst folio-common ausgeführt werden ? Das legt Secrets folio-common.db.secret, folio-common.okapi.secret, folio-common.kafka.secret ... an ?

09.10.
   cd folio-helm-hbz/folio-helm-v2/charts/mod-users
   helm dependency build    (dadurch wird folio-common bekannt oder aktualisiert)
      ...Successfully got an update from the "folio-helm-v2" chart repository
      ...Successfully got an update from the "bitnami" chart repository
      Update Complete. ⎈Happy Helming!⎈
      Saving 1 charts
      Deleting outdated charts
   cd folio-helm-hbz/
   helm template mod-users folio-helm-v2/mod-users -f global-values.yaml > /tmp/hik2
   legt 2 Secrets an:  mod-users-db und  mod-users-kafka
   Ich will aber folio-db-secret aus zalando-pg benutzen, dort definiert in folio.secret.yaml , dort isbd. FOLIO_PASSWORD anstatt DB_PASSWORD .
    => _secret.tpl in folio-common abwandeln; DB_PASSWORD und DB_HOST aus Zalando-Secret holen und aus Values.integrations.db herauswerfen
    Florian / Jason : Helm zieht die Charts nicht von lokal, sondern aus dem Repo. In Dependecies ist das angegeben.
       Entweder ein lokales Repo mit den Änderungen benutzen, oder versuchen, die Änderungen von lokal zu ziehen
       (helm --help etc. studieren) => ./ verwenden

10.10.
   cd folio-helm-hbz/
   helm template mod-users ./folio-helm-v2/charts/mod-users -f global-values.yaml > /tmp/hik2
   gelernt:
     - in folio-commons _secret.tpl einfach ein DB_PASSWORD hinzufügen funktionert nicht: Syntax Fehler bei Konversion von YAML nach JSON
     geplante Vorgehensweise:
     - erzeuge zuert ein eigenes Datenbankgeheimnis folio-db-secret, oder besser db-connect-folio  erl.
     - Erzeuge es mit kubectl apply (es muss nicht Teil eines Helm-Charts sein => doch, neues Chart folio-secrets). Es enthält folgende Werte:
     db-connect Secret key-value pairs:
      DB_CHARSET = UTF-8
      DB_DATABASE = folio
      DB_HOST = folio-zal-pg
      DB_MAXPOOLSIZE = 20
      DB_PASSWORD = folio123   = FOLIO_PASSWORD
      DB_PORT = 5432
      DB_QUERYTIMEOUT = 120000
      DB_USERNAME = folio  erl.
      - diese Werte vielleicht in ein 2. Secret db-connect-okapi
     db-connect-okapi Secret key-value pairs:
      PG_DATABASE = okapi
      PG_HOST = folio-zal-pg
      PG_PASSWORD = okapi25    = OKAPI_PASSWORD
      PG_PORT = 5432
      PG_USERNAME = okapi   erl.
     - Die Werte für die Passwörter müssen automatisch generiert werden, so
         OKAPI_PASSWORD: {{ randAlphaNum 32 | b64enc | quote }} erl.
         FOLIO_PASSWORD: {{ randAlphaNum 32 | b64enc | quote }} erl.
       evtl. ein eigenes Helm-Chart folio-secrets dafür erzeugen erl.
     - benutze diese Geheimnisse dann bei Aufsetzen von postgres, okapi, elasticsearch ?, kafka ? und den Modulen
     - die einzelnen Module erhalten ihr Secret dann über integrations.db.exsitingSecret in der global-values.yaml,
       folio-common legt dann keine weiteren db-Secrets an.
     - das Template folio.secret.yaml in zalando_pg wird nicht mehr benötigt, ich lege folio-db-secret oder äquivalent selber an. erl.
     - um unterschiedliche Variablennamen beim Initialisieren der PG-Datenbank und bei den Modulen zuzulassen,
       holt sich das template zalando-pg:folio.init.job.yaml nicht über "secretRef" sondern über "secretKeyRef", so: erl.
        env:
        - name: FOLIO_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-connect-folio
              key: DB_PASSWORD
        und genau so für OKAPI_PASSWORD. erl. Auch das Repo okapi-helm wird es so machen (dort heißt es intern sogar schon PG_PASSWORD).
     - das Secret okapi kann bleiben, es erhält gar kein Passwort sondern nur eine okapi-config.json ohne Passwort. Das Okapi-Passwort - sowie alle anderen Parameter der okapi-config.json auch noch einmal - werden dem Okapi-Container direkt als ENV-Variable übergeben.

11.10.24
  Neues Helm-Chart folio-secrets angelegt, darin ein Secret db-connect-folio angelegt:
   helm template folio-secrets ./folio-secrets > /tmp/hik3
   helm install -n folio-backend folio-secrets ./folio-secrets

   Meeting d-sys-ops:
   Florian K. : BVB-Modules Deployment hat EIN Template für alle Module. folio-helm-v2 braucht viel mehr Änderungen
                bei Release-Wechsel; alle Modul-Templates müssen geändert werden. Probleme bei Release-Wechsel.
		Die BVB-Charts sind außerdem optimiert für den Anwendungsfall: hunderte von Mandanten. 
		 Jsons YAMLs haben diesen Anwendungsfall nicht.
		Z.Zt. hat BVB-Modules noch keinen Eureka-Support. Florian und Tobias gehen aber davon aus, dass das
           nur 2 zusätzliche Charts werden und leicht integrierbar ist. In diesem Jahr kommt Florian nicht mehr dazu.
	         Uwe: Durch Beschäftigung mit folio-helm-v2 baust du auf jeden Fall das richtige Knowledge auf.

   weiter mit Einrichtung eines 2. Secrets db-connect-okapi erl.

14.10.2024
  Neuen PC zu GitLab-Repo hinzufügen: Gehe nach http://quaoar4.hbz-nrw.de:8929/-/user_settings/ssh_keys/
  Neues Secret db-connect-okapi erstellt, installieren mit
    helm upgrade -n folio-backend folio-secrets ./folio-secrets
    kubectl delete secret folio-db-secret
        secret "folio-db-secret" deleted (es ist aber noch da)
    helm upgrade -n folio-backend zal-pg ./zalando-pg -f global-values.yaml
   Mon Oct 14 18:21:59 UTC 2024 Creating SUPERUSER pgsuperuser ...
ERROR:  role "pgsuperuser" already exists

    helm uninstall -n folio-backend zal-pg ./zalando-pg
    helm install -n folio-backend zal-pg ./zalando-pg -f global-values.yaml
        container "create-pg-superuser" in pod "create-pgsuperuser-9xnr7" is waiting to start: CreateContainerConfigError
        Error: secret "postgres.folio-zal-pg.credentials.postgresql.acid.zalan.do" not found 
    Problem mit Service-Namen vs. Hostnamen ; Name: postgresql, Host: folio-zal-pg
       jetzt kommt Creating SUPERUSER pgsuperuser ...
               psql: error: could not translate host name "folio-zal-pg" to address: Name or service not known
       => den Service wieder umbenannt in folio-zal-pg
     Mon Oct 14 18:56:53 UTC 2024 Creating SUPERUSER pgsuperuser ...
    CREATE ROLE
     Mon Oct 14 18:56:53 UTC 2024 User created!
       von folio-init-job aber:
     ERROR:  role "okapi" already exists
     - mal mit initDb = false starten
   helm install -n folio-backend zal-pg ./zalando-pg -f global-values.yaml
    NAME: zal-pg
    LAST DEPLOYED: Mon Oct 14 21:06:29 2024
    NAMESPACE: folio-backend
    STATUS: deployed
    REVISION: 1
    TEST SUITE: None
      und
    Mon Oct 14 19:07:26 UTC 2024 Creating databases okapi and folio ...   ??? Hatte ich das jetzt nicht deaktiviert ??
    Mon Oct 14 19:07:26 UTC 2024 Running: psql -w -v ON_ERROR_STOP=1 -h folio-zal-pg -p 5432 -U postgres ...
    CREATE ROLE
    CREATE DATABASE
    CREATE ROLE
    CREATE DATABASE
    Mon Oct 14 19:07:26 UTC 2024 Init script is completed
      und
    Mon Oct 14 19:07:41 UTC 2024 Creating SUPERUSER pgsuperuser ...
    CREATE ROLE
    Mon Oct 14 19:07:41 UTC 2024 User created!

   OK, postrges ist efolgreich re-deployed mit eigenen DB-Secrets.
   Elasticsearch und Kafka sind noch deployed, letzteres im Namensraum folio-staging-v1-kafka. Bei denen muss man nichts ändern.
      Deren Konfigurationen sind in ~/folio-hbz, ohne Datenbankbezug.
   Jetzt weiter mit Okapi. Dann die Module, zuerst nur mods-auth. Dann Anlage eines Mandanten (tenant register).

17.10.2024
    Indexdata Okapi Helm Chart auf meine DB-Secrtes angepasst. Okapi liest aus dem vorhandenen Secret db-connect-okapi.
    Neu-Deployment von Okapi:
    cd okapi-helm
    helm template -f ../global-values.yaml okapi ./okapi > /tmp/hik2
    helm upgrade -n folio-backend  -f ../global-values.yaml okapi ./okapi
      Release "okapi" has been upgraded. Happy Helming!
      NAME: okapi
      LAST DEPLOYED: Fri Oct 18 11:13:15 2024
      NAMESPACE: folio-backend
      STATUS: deployed
      REVISION: 17
      NOTES:
      1. Get the application URL by running these commands:
        export NODE_PORT=$(kubectl get --namespace folio-backend -o jsonpath="{.spec.ports[0].nodePort}" services okapi)
        export NODE_IP=$(kubectl get nodes --namespace folio-backend -o jsonpath="{.items[0].status.addresses[0].address}")
        echo http://$NODE_IP:$NODE_PORT

18.10.2024
  mods-auth wieder zum Laufen kriegen
  Umstellung der Secrets von "folio-db-secret" auf meine DB-Secrets (db-connect-folio, db-connect-okapi)
  cd ~/folio/folio-helm-hbz
  # Achtung: helm upgrade funktioniert nicht; vom init Job kommt Meldung "field is immutable"; man MUSS zunächst De-installieren:
  helm uninstall -n folio-backend mods-auth
  helm install -n folio-backend mods-auth modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-auth.yaml
    NAME: mods-auth
    LAST DEPLOYED: Fri Oct 18 11:25:17 2024
    NAMESPACE: folio-backend
    STATUS: deployed
    REVISION: 1
    TEST SUITE: None
    
  Es kommt die alte Meldung, von Job init-mod-authtoken
    -- Waiting for file kube-scripts/init-module.sh...
    testing if kube-scripts/init-module.sh exists...done
    lrwxrwxrwx 1 root root 21 Oct 18 13:09 kube-scripts/init-module.sh -> ..data/init-module.sh
    /usr/src/app/docker-entrypoint.sh: line 15: ./kube-scripts/init-module.sh: cannot execute: required file not found
    
    Florian hat das Skript editiert; neues Tag 
    Florian Kreft vor 25 Minuten     debian-v1.3-debug2
    bei Florian wird das Skript aber ausgeführt:

Florian Kreft vor 3 Minuten
-- Waiting for file kube-scripts/init-module.sh...
15:08 Uhr
-- Waiting for file kube-scripts/init-module.sh...
testing if kube-scripts/init-module.sh exists...done
lrwxrwxrwx 1 root root 21 Oct 18 13:08 kube-scripts/init-module.sh -> ..data/init-module.sh
ping okapi....
PING okapi.folio-clone-03.svc.foliotest.local (10.237.64.190) 56(84) bytes of data.
From okapi.folio-clone-03.svc.foliotest.local (10.237.64.190) icmp_seq=1 Destination Port Unreachable
--- okapi.folio-clone-03.svc.foliotest.local ping statistics ---
1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms
-- Getting superuser token...done.
-- Checking if okapi is ready...done
--- deploying module mod-agreements-6.1.4 to http://okapi:9130 ...
-- testing whether module mod-agreements-6.1.4 is already introduced to http://okapi:9130 ...already introduced.
--- enabling module mod-agreements-6.1.4 to http://okapi:9130 ...
-- testing whether module mod-agreements-6.1.4 is already enabled in http://okapi:9130 ...already enabled.
-- removing module mod-agreements-6.1.4 from http://okapi:9130 ...done.
-- enabling module mod-agreements-6.1.4 to http://okapi:9130 ...{"srvcId":"mod-agreements-6.1.4","instId":"mod-agreements-6.1.4","url":"http://mod-agreements-6-1-4-2023r2csp1:8080"}
500 (recvAddress(..) failed: Connection reset by peer)
  (er bekommt andere Fehlermeldung; aber das Skript wird ausgeführt.)

   Zur Kubernetes-Struktur: 
   -----------------------
   das Deployment geht nur auf grün, wenn der init Job erfolgreich war.
   das Deployment starte die Pods (falls es "grün" ist).
   Stateful Sets werden - zumindest bei okapi-helm - nur im Cluster Modus angelegt. Okapi starte ich aber derzeit noch nicht im Cluster Modus (das wäre Hazelcast).
   Außerdem gibt es Services. Die Module haben einen. Die postgres, Okapi und elasticsearch auch.

18.10.2024
  helm uninstall -n folio-backend mods-auth
  helm install -n folio-backend mods-auth modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-auth.yaml
    -- Waiting for file kube-scripts/init-module.sh...
    testing if kube-scripts/init-module.sh exists...done
    lrwxrwxrwx 1 root root 21 Oct 28 11:50 kube-scripts/init-module.sh -> ..data/init-module.sh
    ping okapi....
    PING okapi.folio-backend.svc.cluster.local (10.43.126.141) 56(84) bytes of data.
    --- okapi.folio-backend.svc.cluster.local ping statistics ---
    1 packets transmitted, 0 received, 100% packet loss, time 0ms
    -- Getting superuser token...failed: 000 ()
    okapi still unsecured?
    continuing with empty superusertoken...
    -- Checking if okapi is ready...ERROR, returncode 000
       Analyse: der init-Job hat die configMapRef nicht erhalten.
       die configMap mod-authtoken-2-14-1-2023r2csp2-env mal nicht löschen.
       Warten bis die Jobs von selber verschwunden sind.
       Dann die Deployments: mod-permissions, mod-users, mod-login, mod-authtoken, mal löschen. Die dazu gehörenden Services ebenfalls löschen.
       (muss man nicht)
  helm upgrade -n folio-backend mods-auth modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-auth.yaml
     => envFrom Syntax in init.job.yaml geändert: envFrom darf nicht 2x erscheinen, das zweite Vorkommen überschreibt das erste !!
     Returnwert des okapi ready check ist jetzt: "000"[ ]"200" => Zerifikatsproblem ?
     Versuche mit okapi.secureSuperTenant: True in global-values.yaml (das ist egal, ich habe ja Okapi von IndexData deployed).
     Hole supertenant: superuser und Passwort aus den Secrets. (das Secret secure-supertenant unter modules/templates wird nicht benötigt).
     Das IndexData Okapi ist nicht abgesichert.
     Entferne alle Header "X-Okapi-Tenant" aus dem init.sh Skript
     Noch Probleme mit Gänsefüßchen: "200" vs. 200
     Jetzt geht:
       enabling module mod-permissions-6.4.0 to http://okapi:9130 ...{"srvcId":"mod-permissions-6.4.0","instId":"mod-permissions-6.4.0","url":"http://mod-permissions-6-4-0-2023r2csp2:8081"}
done
      enabling module mod-users-19.2.2 to http://okapi:9130 ...{"srvcId":"mod-users-19.2.2","instId":"mod-users-19.2.2","url":"http://mod-users-19-2-2-2023r2csp2:8081"}
done
    jedoch
      400 (Missing dependency: mod-login-7.10.1 requires users: 15.4 16.0 Missing dependency: mod-login-7.10.1 requires user-tenants: 1.0)
      400 (Missing dependency: mod-authtoken-2.14.1 requires permissions: 5.3 Missing dependency: mod-authtoken-2.14.1 requires users: 15.0 16.0)
   im 2. Versuch, ohne etwas verändert zu haben:
      enabling module mod-login-7.10.1 to http://okapi:9130 ...{"srvcId":"mod-login-7.10.1","instId":"mod-login-7.10.1","url":"http://mod-login-7-10-1-2023r2csp2:8081"}
done
      enabling module mod-authtoken-2.14.1 to http://okapi:9130 ...{"srvcId":"mod-authtoken-2.14.1","instId":"mod-authtoken-2.14.1","url":"http://mod-authtoken-2-14-1-2023r2csp2:8081"}
done
    Die init Jobs sind jetzt "completed", aber die Deployments sind rot und die Pods bleiben auf "Pending"
  helm uninstall -n folio-backend mods-auth
  helm install -n folio-backend mods-auth modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-auth.yaml
  Die Deployments bleiben rot. Statusmeldungen sind:
      status:
  conditions:
    - lastTransitionTime: '2024-10-28T16:52:04Z'
      lastUpdateTime: '2024-10-28T16:52:04Z'
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: 'False'
      type: Available
    - lastTransitionTime: '2024-10-28T16:52:04Z'
      lastUpdateTime: '2024-10-28T16:52:04Z'
      message: ReplicaSet "mod-authtoken-2-14-1-2023r2csp2-66c8598d9b" is progressing.
      reason: ReplicaSetUpdated
      status: 'True'
      type: Progressing
  observedGeneration: 1
  replicas: 1
  unavailableReplicas: 1
  updatedReplicas: 1

SysOps 06.11.24
  don't start on control planes
  "Taints" and tolerations for the control planes
  system components have tolerations on the taints
  most control planes have those taints automatically
  most pods only run on worker nodes
  "taint and tolerate" those worker nodes
  node-role.kubernetes.io/control-plane=NoSchedule
  Ingolf: gitlab.bib-bvb.de kann ich weder forken noch klonen, dazu benötigt man ein Konto bei diesem gitlab.
          Daher kann ich Änderungen weder pullen noch pushen.
          Bei https://gitlab.bib-bvb.de/folio-public/folio-tools habe ich den Container direkt herunter gezogen 
          (das Template macht das), da ich keine Änderungen an diesem Container benötige.
          Bei folio-helm geht das aber nicht, da ich zahlreiche Änderungen vorgenommen habe, um meine Infrastruktur zu integrieren
          (anderes Okapi verwendet [von Index Data], eigene Secrets/Passwortverwaltung, andere Konfiguration von postgres [values.yaml]).

07.11.2024
  helm uninstall -n folio-backend mods-auth
  helm install -n folio-backend mods-auth modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-auth.yaml
  Die Jobs sind jetzt erfolgreich (enabling module mod-authtoken-2.14.1 to http://okapi:9130 ...).
  Die Deployments sind grün:
    message: >-
        ReplicaSet "mod-permissions-6-4-0-2023r2csp2-6c8dcf4df5" has
        successfully progressed.
      reason: NewReplicaSetAvailable
      status: 'True'
    Ausnahme: mod-users:
       message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: 'False'
  Die Pods sind auf grün und loggen nur INFO-Meldungen.
    INFO Succeeded in deploying verticle 
    Ausnahme: mod-users:
      15:45:13 WARN  ClientUtils          Couldn't resolve server kafka:9092 from bootstrap.servers as DNS resolution failed for kafka
      15:46:48 ERROR RestVerticle         Failed to construct kafka consumer
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
 at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:830) ~[mod-users-fat.jar:?]
     ...
      15:46:48 ERROR ?                    Failed in deploying verticle

   Der Container mod-users braucht KAFKA_HOST, KAFKA_PORT und bekommt es aus der ConfigMap mod-users-*-env .
   Diese ConfigMap wird durch das Template backend-module.configmap.yaml erzeugt. Dieses wiederum holt sich die Werte aus
   global-values.yaml : Values.global.kafka.serviceName, Values.global.kafka.port.
   Aber was muss da bei mir drin stehen ? Warum nicht kafka:9092 ?
   s. meine Installationsnotizen in kube/
   "Kafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster:
    kafka.folio-staging-v1-kafka.svc.cluster.local"

   Kafka auch im Namespace folio-backend installiert, siehe in kube/ks.install.txt .
     helm upgrade -n folio-backend mods-auth modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-auth.yaml
   => Deployment und Pods für mod-users sind jetzt grün.

   Weiter mit "secure supertenant", hier https://gitlab.bib-bvb.de/folio-public/folio-helm/#secure-supertenant => erst mal nicht gemacht
     helm install -n folio-backend secure-st secure-supertenant/ -f global-values.yaml

   Installation der restlichen Module: https://gitlab.bib-bvb.de/folio-public/folio-helm/#other-modules
   **********************************
     helm install -n folio-backend mods modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-no-auth.yaml
   NAME: mods
   LAST DEPLOYED: Thu Nov  7 19:05:04 2024
   NAMESPACE: folio-backend
   STATUS: deployed
   REVISION: 1
   TEST SUITE: None

   Mandant und Frontend
   ====================
   Weiter hier: https://gitlab.bib-bvb.de/folio-public/folio-helm/#create-tenant-and-stripes-deployment
   Siehe https://gitlab.bib-bvb.de/folio-public/stripes-build (ein eigenes Repo für Stripes)
    - passendes Tag dieses Repos wählen: poppy-csp2-complete-2  => eingetragen in tenant/values.yaml image.stripesTag
    - Umgebungsvariablen setzen (werden verwendet in strips.config.js):
     $OKAPI_URL_INGRESS The Okapi URL as seen from the end-user
     $TENANT Tenant ID
     $LOG_PREFIX Log Prefix for Stripes
     $WELCOME_MESSAGE  Welcome Message
     $PLATFORM_NAME  Platform Name
     $LOGO_ALT_TXT Alternative Text for the logo
     $LOGIN_BACKGROUND_COLOR Background color of the login screen
     $NAV_BACKGROUND_COLOR Background Color of navigation bar
     $LOGO_URL
     $FAVICON_URL
          ==> siehe in tenant/templates/stripes.configmap.yaml
     "create new stripes branch in gitlab ( https://gitlab.bib-bvb.de/folio/platform-complete ) from branch with correct version (e.g. lotus-base) and adjust values" => das kann ich nicht machen / keine Berechtigung
     - gezogen wird aber das hier, und das ist öffentlich: https://gitlab.bib-bvb.de/folio-public/stripes-build; davon das Tag "stripesTag"
   - Copy and adapt tenant/values.yaml   erl. / folioToolsTag angepasst
   - Installiere Mandanten (?)
      helm install -n folio-backend folio-r2-2023-csp2 tenant/ -f global-values.yaml -f tenant/values.yaml -f modules/valuesd/2023r2csp2-all.yaml 
      .. verschiedene Skripte im files-Verzeichnis; diese müssen kennen:
         OKAPI_URL (http://okapi.%s:9130), FOLIO_ADMIN_USERNAME (diku_admin), FOLIO_ADMIN_PASSWORD (admin), FOLIO_TENANTID (diku) u.a.
                                      =          ADMIN_USERNAME                     ADMIN_PASSWORD                TENANT
                                 %s = <Namespace>
         zur Kommunikation mit OKAPI wird der OKAPI-CommandLineInterface (CLI) benutzt --- nicht curl.

     die Templates unter tenant/ mal durchgehen:
       - wo ist Values.tenants.clientAdminAccount ? -> hinzugefügt "admin"
       - erst einmal ohne Ingress (ingress: false, ist auch der Default)
       - erst einmal ohne EMAIL-Konfiguration (ist auskommentiert in tenant/values.yaml)

    helm install -n folio-backend folio-r2-2023-csp2 tenant/ -f global-values.yaml -f tenant/values.yaml -f modules/valuesd/2023r2csp2-all.yaml 
    Logs von Job  folio-r2-2023-csp2-register-diku 
   Starting to register tenant diku with okapi at http://okapi:9130...
   Getting superuser token...testing whether module folio_oa-2.0.0 is already registered to http://okapi:9130 ...not found.
   fetching module-descriptor from http://folio-registry.aws.indexdata.com for folio_oa-2.0.0 ...done
   registering module folio_oa-2.0.0 to http://okapi:9130 ...done
   testing whether module folio_acquisition-units-5.0.0 is already registered to http://okapi:9130 ...not found.
   fetching module-descriptor from http://folio-registry.aws.indexdata.com for folio_acquisition-units-5.0.0 ...done
   registering module folio_acquisition-units-5.0.0 to http://okapi:9130 ...400 (Missing dependency: folio_acquisition-units-5.0.0 requires acquisitions-units: 1.1)
      - Pods bleiben rot
         - außerdem Fehlermeldung von mod-data-import: storageclass.storage.k8s.io "longhorn-tworep" not found (!)

08.11.24
  Uwe: wg. der fehlenden Dependencies: Einfach mehrfach probieren; das könnten Race Conditions sein. 
  Uwe: Nur mit dem Ingress kommst du von außen ans Okapi dran, sonst nur intern in Kubernetes. Ich würde auch den Ingress installieren.
   helm uninstall -n folio-backend folio-r2-2023-csp2

13.11.24
  IK: Das Interface acquisitions-units: 1.1  wird von mod-orders bereit gestellt ("provides").
  Zuerst die Backend-Module anmelden, dann die Frontend-Module.
  Probleme bei Anmeldung (POST an /_/discovery) von mod-inventory-update-3.2.1
  Bei "ReplicaSets" gucken, was das Problem ist:
    ERROR StatusLogger Log4j2 could not find a logging implementation. Please add log4j-core to the classpath. Using SimpleLogger to log to the console...
  Die Module noch einmal installieren;
    helm upgrade -n folio-backend mods modules/ -f global-values.yaml -f modules/valuesd/2023r2csp2-no-auth.yaml
    # das erledigt das Deployment der Module. Die Images der Module werden nacheinander an Kubernetes übergeben und Kubernetes macht 
      je ein Deployment und einen Pod daraus (außerdem noch ein Replica Set). Die Namen der Images stehen in valuesd/2023r2csp2-all.yaml .
      Das Deployment macht das Template modules/template/backend-module.deployment.yaml .
      In modules/files/init-module.sh wird das Modul am Okapi-Discovery angemeldet.
  mod-inventory-update läuft fehlerfrei durch:
   bei "Jobs", dann das log angucken (init-mod-..):
   -- enabling module mod-inventory-update-3.2.1 to http://okapi:9130 ...{"srvcId":"mod-inventory-update-3.2.1","instId":"mod-inventory-update-3.2.1","url":"http://mod-inventory-update-3-2-1-2023r2csp2:8080"}
done

    Nun erneut beim Mandanten anmelden:
    ***********************************
    helm upgrade -n folio-backend folio-r2-2023-csp2 tenant/ -f global-values.yaml -f tenant/values.yaml -f modules/valuesd/2023r2csp2-all.yaml 
     Bei Jobs das Log angucken
      ==> mod-inventory-update-3.2.1 konnte jetzt erfolgreich angemeldet werden, jetzt Problem bei mod-patron-blocks-1.9.0
       Wieder Module "upgraden":
     ... -- enabling module mod-patron-blocks-1.9.0 to http://okapi:9130 ...{"srvcId":"mod-patron-blocks-1.9.0","instId":"mod-patron-blocks-1.9.0","url":"http://mod-patron-blocks-1-9-0-2023r2csp2:8081"}
done
     und jetzt wieder der Mandant (Job "...-upgrade-diku"): Succeeded waiting for backend modules
     bei den Frontend-Modulen fällt der Job dann ganz am Ende auf die Nase:
    testing whether module folio_users-10.0.5 is already registered to http://okapi:9130 ...already registered.

    helm upgrade -n folio-backend folio-r2-2023-csp2 tenant/ -f global-values.yaml -f tenant/values.yaml -f modules/valuesd/2023r2csp2-all.yaml
  Jetzt wird "install_modules()" ausgeführt. POST an Okapi's endpoint /install .
Installing modules for tenant ...Installing modules --- 3 tries left ....
400
Installing modules --- 2 tries left ....
400
Installing modules --- 1 tries left ....
400
Installing modules --- 0 tries left ....
400 (POST request for mod-permissions-6.4.0 /_/tenant failed with 400: FATAL: sorry, too many clients already (53300))
  Analyse: Postgres-Fehler ! "Your code opened up more than the allowed limit of connections to the postgresql database."

   Der Stripes-Container läuft auch nicht:
   lastTransitionTime: '2024-11-08T18:41:50Z'
      message: 'containers with unready status: [folio-r2-2023-csp2-stripes-diku]'
      reason: ContainersNotReady
      status: 'False'
   Das ist Teil von tenant/templates/stripes.deployment.yaml
      container "folio-r2-2023-csp2-stripes-diku" in pod "folio-r2-2023-csp2-stripes-diku-668f4ff7bb-7dqt8" is waiting to start: trying and failing to pull image
    dieses Image: gitlab.bib-bvb.de:5050/folio-public/stripes-build:poppy-csp2-complete-2
    Ist da etwas der Build fehlgeschlagen ? s. hier: https://gitlab.bib-bvb.de/folio-public/stripes-build/-/commits/poppy-csp2-complete-2 (rotes Kruez)

D-Sys-Ops 15.11.2024
   Der Stripes-Container ist schon gebaut (bzw. das Bauen ist fehlgeschlagen auf Florian's Seite). Man muss den Stripes-Container nur 1x bauen.
   Die Informationen über den Mandanten (URL, Namen), werden dann injiziert.

   max_connections bei der postgres hochsetzen:
   1. in ~/folio-helm-hbz/zalando-pg/values.yaml eintragen: max_connections: "1200"
   2. helm upgrade -n folio-backend -f global-values.yaml -f zalando-pg/values.yaml zal-pg ./zalando-pg
      - das schlägt zwar fehl, macht aber nichts, ist ein "Post-Upgrade-Job".

   In einen postgres-Container gehen, um zu gucken, ob der Parameter hoch gesetzt wurde:
     Pods -> folio-zal-pg-1 ; dann Klick auf den Pfeil -> Konsole öffnet sich: root@folio-zal-pg-1:/home/postgres
     psql -U folio; SELECT * FROM pg_settings WHERE name = 'max_connections';
           name       | setting | unit |                       category                       |                     short_desc                     | extra_desc |  context   | vartype |    source    | min_val | max_val | enumvals | boot_val | reset_val | sourcefile | sourceline | pending_restart 
-----------------+---------+------+------------------------------------------------------+----------------------------------------------------+------------+------------+---------+--------------+---------+---------+----------+----------+-----------+------------+------------+-----------------
 max_connections | 1200    |      | Connections and Authentication / Connection Settings | Sets the maximum number of concurrent connections. |            | postmaster | integer | command line | 1       | 262143  |          | 100      | 1200      |            |            | f
(1 row)

   Anders: Gucken, mit Welchen Werten zal-pg deployed wurde; das zeigt aber nur die global-values.yaml:
     helm -n folio-backend get values zal-pg
   3. Jetzt den Mandanten aktualisieren:
    helm upgrade -n folio-backend folio-r2-2023-csp2 tenant/ -f global-values.yaml -f tenant/values.yaml -f modules/valuesd/2023r2csp2-all.yaml
    => install_modules() wird jetzt ohne Fehler ausgeführt.

   Gucken, was ich in dem Namensraum alles deployed habe: helm -n folio-backend list

   Wenn der Stripes-Container läuft:
   Okapi und Stripes als Node Port Service einrichten.
   ***************************************************
   Okapi und Stripes als Node Port Service. Dann sucht sich das System automatisch einen Service und Port (< 32.000).
   Das ist dann von außen zugänglich. Und zwar über den Node. Ich gebe dann den Namen von mindestens einem der Nodes an.
   Optimalerweise sind alle 3 Nodes gültig, den Reverse Proxy so konfigurieren.
   Intern auf einen http Node Port Service weiterleiten.
   Uwe: Ein Load Balancer mit automatischem Fail over (ist komplizerter)
   Florian: Einfacher dem Reverse Proxy sagen, diese drei Nodes sind gültig.
   Tobias: Metal-LB; dann benötige ich keinen Reverse Proxy.

