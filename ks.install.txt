# 19.08.2024
# Testen
  helm template -f global-values.yaml zalando-pg
# Installieren
  helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
  # helm install --debug --dry-run -n folio-backend -f global-values.yaml zal-pg ./zalando-pg  -- ähnlich wie helm template, macht nichts, gibt das YAML aus
  kubectl describe -n folio-backend pods
# Löschen
  # helm delete -n folio-backend zal-pg
  helm uninstall -n folio-backend zal-pg

  Ein Secret liegt in einer privaten Container-Registry
  Anleitung: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  docker login
  A Kubernetes cluster uses the Secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.
  If you already ran docker login, you can copy that credential into Kubernetes:
  kubectl create secret generic regcred --from-file=.dockerconfigjson=$HOME/.docker/config.json --type=kubernetes.io/dockerconfigjson
    secret/regcred created
  To understand the contents of the regcred Secret you created, start by viewing the Secret in YAML format:
    kubectl get secret regcred --output=yaml
  To understand what is in the .dockerconfigjson field, convert the secret data to a readable format:
    kubectl get secret regcred --output="jsonpath={.data.\.dockerconfigjson}" | base64 --decode
  Das gleiche Secret habe ich auch noch einmal mit anderem Namen im Namensraum "folio-backend" erzeugt:
    kubectl get secret -n folio-backend folio-hbz-dockerhub-key --output="jsonpath={.data.\.dockerconfigjson}" | base64 --decode

  To pull the image from the private registry, Kubernetes needs credentials. 
  The imagePullSecrets field in the configuration file specifies that Kubernetes should get the credentials from a Secret named gitlab-folio-group .
  The image is: gitlab.bib-bvb.de:5050/folio/folio-tools:debian-v1.3
  In Job folio.init.job.yaml
  In turn, the image itself need environment variables from a secret key ref:
         env:
        - name: PGPASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres.folio-release-name.credentials.postgresql.acid.zalan.do
              key: password
   Weiterlesen hier https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/  "What's next"

  Für den Moment einfach nur postgresql auf das Cluster bringen, ohne die Secrets, User- und Init-Jobs/Scripts; so:
    kubectl -n folio-backend -f folio-zal-pg.yml apply
      Warning ...
      postgresql.acid.zalan.do/folio-zal-pg configured

22.08.2024
  https://kubernetes-io.translate.goog/docs/concepts/containers/images/?_x_tr_sl=auto&_x_tr_tl=de&_x_tr_hl=de#using-a-private-registry
    Lesen: Angeben von ImagePullSecrets auf einem Pod  (bekannt)
  aus folio-public/folio-tools ein Image bauen  ==> Das brauche ich nur, wenn ich nicht von bvb/folio-public herunter ziehen kann
    docker build https://gitlab.bib-bvb.de/folio-public/folio-tools
       Error response from daemon: dockerfile parse error line 5: unknown instruction: <!DOCTYPE
   anders: Repo klonen nach ~/folio-tools, dann
     cd ~/folio-tools
     docker build .
      Sending build context to Docker daemon  70.66kB
      Step 1/6 : FROM debian:bookworm-slim
      Get "https://registry-1.docker.io/v2/": dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:48605->[::1]:53: read: connection refused
---------------
   Erneuter Versuch zal-pg Deployment, nachdem Florian K. das images in der values.yml auf "folio-public" geändert hat
      helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
         error: INSTALLATION FAILED: failed post-install: timed out waiting for the condition
   create-pgsuperuser schlägt fehl (FAIL), Log ist dieses (das wird immer wieder versucht):
   - Waiting for file kube-scripts/create-pgsuperuser.sh...
    testing if kube-scripts/create-pgsuperuser.sh exists...done
    Thu Aug 22 17:43:12 UTC 2024 Creating SUPERUSER pgsuperuser ...
    psql: error: connection to server at "folio-zal-pg" (10.43.166.10), port 5432 failed: Connection refused
    ********************************************************************************************************
     Is the server running on that host and accepting TCP/IP connections?
         kubectl describe -n folio-backend pods
    Das Herunterziehen von folio-public geht jetzt !
   Beim Job "create-pgsuperuser" kommen jetzt folgende Events:
Events:
  Type    Reason     Age        From                  Message
  ----    ------     ----       ----                  -------
  Normal  Scheduled  <unknown>                        Successfully assigned folio-backend/create-pgsuperuser-snk6t to folio-k8s13
  Normal  Pulled     6m34s      kubelet, folio-k8s13  Container image "gitlab.bib-bvb.de:5050/folio-public/folio-tools:debian-v1.3" already present on machine
  Normal  Created    6m34s      kubelet, folio-k8s13  Created container create-pg-superuser
  Normal  Started    6m34s      kubelet, folio-k8s13  Started container create-pg-superuser
     Das passiert 6x (warum ?)
    - das Secret postgres.folio-zal-pg.credentials.postgresql.acid.zalan.do mit dem Passwort für den db-user postgres wurde angelegt.

      - "folio-zal-pg" steht auf Status: Pending und Grund: 0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.
                          - der Pod folio-zal-pg hat VolumeMounts und Volumes, aber sind die richtig definiert ?
                            die haben weder "name" noch "claimName" => hier weiter
      - aber auch das PVC "pgdata-folio-zal-pg" steht auf Status: Pending.
                -- der PVC sieht richtig aus, aber irgendwie gibt es kein Volume und keinen VolumeMount dazu. Siehe Cluster -> Persistent Volumes
                        die StorageClass local-path hat übrigens den Provisioner rancher.io
        das (folio-zal-pg und pgdate-folio-zal-pg) sind beides Services und ihre Manifestationen (YAML) sind im Dashboard unter Service - Services einsehbar.

23.08.
  in folio-zal-pg Pid
       volumeMounts:
        - mountPath: /home/postgres/pgdata
          name: pgdata
	  volumes:
    - name: pgdata
      persistentVolumeClaim:
        claimName: pgdata-folio-zal-pg-0
    - volumeName: pvc*   fehlt

    kind: postgres - Custom Resource Definition
     kubectl get crd

     nach Anpassen der CPUs / RAM, hat Pod folio-zal-pg jetzt den Status "Running" !
     Im Log kommen aber noch Fehler:
     Success. You can now start the database server using:
    /usr/lib/postgresql/13/bin/pg_ctl -D /home/postgres/pgdata/pgroot/data -l logfile start
2024-08-23 12:48:57.610 GMT [208] LOG:  invalid value for parameter "lc_monetary": "de_DE.UTF-8"
2024-08-23 12:48:57.610 GMT [208] LOG:  invalid value for parameter "lc_numeric": "de_DE.UTF-8"
2024-08-23 12:48:57.610 GMT [208] LOG:  invalid value for parameter "lc_time": "de_DE.UTF-8"
2024-08-23 14:48:57.610 CEST [208] FATAL:  configuration file "/home/postgres/pgdata/pgroot/data/postgresql.conf" contains er

      Nützliche Befehle:
       helm -n folio-backend list   - eine Liste, was in dem Namespace alles deployed ist.
       helm list - dasselbe für alle Namespaces
      mal das Log vom postgres-operator verfolgen:
       kubectl --namespace=default logs  "postgres-operator-8bc78d9f8-tzhhm"
      die verwendete Manifestation des Helm-Charts für zalando-pg, angereicherte Version:
       kubectl get -o yaml postgresqls.acid.zalan.do
       kubectl get postgresqls.acid.zalan.do   - Basisinformationen; was läuft
       kubectl get crd - listet Custom Resource Definitionen auf ("postgresql" von Zalando ist z.B. eine solche!)


      Florian: Jetzt noch einmal starten:
 helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
    - bei Job create-pgsuperuser kommt wieder "connection refused"
    Error: INSTALLATION FAILED: failed post-install: 1 error occurred:
        * job create-pgsuperuser failed: BackoffLimitExceeded
   -> evtl. noch einmal alles löschen und die "lc_monetary" ... Fehler in postgresql.conf beheben

26.08.2024
  UTF-8 => utf8 in den Parametern lc_monetary etc. (in values.yaml)
  -- Waiting for file kube-scripts/create-pgsuperuser.sh...
  testing if kube-scripts/create-pgsuperuser.sh exists...done
  Mon Aug 26 10:57:52 UTC 2024 Creating SUPERUSER pgsuperuser ...
  psql: error: connection to server at "folio-zal-pg" (10.43.21.168), port 5432 failed: Connection refused
   Is the server running on that host and accepting TCP/IP connections?

   Jetzt die Parameter entfernt (lc_monetary, ...)
   - in Datenbanklog kommen CREATE und EXEC Anweisungen (gut)
  2024-08-26 11:10:34,381 WARNING: Could not activate Linux watchdog device: Can't open watchdog device: [Errno 2] No such file or directory: '/dev/watchdog'
  - der Job create-pgsuperuser schlägt weiterhin fehl (Verbindung zur Datenbank kann nicht hergestellt werden)
  - das persistente Volume für die pg-Datenbank wurde definitiv erfolgreich zur Verfügung gestellt. Die Datenbank wurde hier angelegt:
    root@folio-k8s13:/var/lib/rancher/k3s/storage/pvc-d016e739-d9db-4634-9182-13b98c9eb19c_folio-backend_pgdata-folio-zal-pg-0/pgroot/
  Jetzt kommen Warnungen im Postgres Log: 
  2024-08-26 11:10:36,057 WARNING: Kubernetes RBAC doesn't allow GET access to the 'kubernetes' endpoint in the 'default' namespace. Disabling 'bypass_api_service'.
  2024-08-26 11:10:36,070 WARNING: postgresql parameter listen_addresses=* failed validation, defaulting to None
  2024-08-26 11:10:36,071 WARNING: postgresql parameter wal_keep_segments=0 failed validation, defaulting to 8
  2024-08-26 11:10:36,071 WARNING: postgresql parameter wal_log_hints=off failed validation, defaulting to on

  "uninstall" anstatt "delete" verwenden:
    helm uninstall -n folio-backend zal-pg
    release "zal-pg" uninstalled
Job zal-pg-folio-init-job geht jetzt:
  -- Waiting for file kube-scripts/folio-db-init.sh...
testing if kube-scripts/folio-db-init.sh exists...done
Mon Aug 26 11:44:55 UTC 2024 Creating databases okapi and folio ...
Mon Aug 26 11:44:55 UTC 2024 Running: psql -w -v ON_ERROR_STOP=1 -h folio-zal-pg -p 5432 -U postgres ...
CREATE ROLE
CREATE DATABASE
CREATE ROLE
CREATE DATABASE
Mon Aug 26 11:44:55 UTC 2024 Init script is completed

Job create-pgsuperuser war jetzt auch erfolgreich:
-- Waiting for file kube-scripts/create-pgsuperuser.sh...
testing if kube-scripts/create-pgsuperuser.sh exists...done
Mon Aug 26 11:44:52 UTC 2024 Creating SUPERUSER pgsuperuser ...
CREATE ROLE
Mon Aug 26 11:44:52 UTC 2024 User created!

Jetzt war erfolgreich:
helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
NAME: zal-pg
LAST DEPLOYED: Mon Aug 26 13:44:51 2024
NAMESPACE: folio-backend
STATUS: deployed
REVISION: 1
TEST SUITE: None

Jetzt läuft erfolgeich: Pods folio-zal-pg-0, folio-zal-pg-1 im Namespace folio-backend; mit den 4 Warnungen wie oben genannt.
Weiter mit Installation Okapi.

30.08.2024
Versuche, Okapi zu installieren
Versuche, offizielles Image von folio-org zu ziehen, dieses hier: https://hub.docker.com/r/folioci/okapi
  cd ~/folio-helm-hbz
  helm install -n folio-backend -f global-values.yaml okapi ./okapi
  helm uninstall -n folio-backend okapi
 Normal   Pulled     34s                kubelet            Successfully pulled image "folioorg/okapi:latest" in 815ms (815ms including waiting). Image size: 100319551 bytes.
  Warning  BackOff    6s (x6 over 79s)   kubelet            Back-off restarting failed container okapi in pod okapi-5f79959657-z6tl2_folio-backend(d0e8fd4c-cf0d-49d6-85e7-5ecc7b3d9005)
  Florian: Besser  Helm-Chart von IndexData verwenden.
 Uwe 
   podman pull gitlab.bib-bvb.de:5050/folio-public/okapi:v5.2.0
   Trying to pull gitlab.bib-bvb.de:5050/folio-public/okapi:v5.2.0...
   Error: initializing source docker://gitlab.bib-bvb.de:5050/folio-public/okapi:v5.2.0: Requesting bearer token: invalid status code from registry 403 (Forbidden)
   
Florian
https://github.com/indexdata/okapi-helm/tree/main
  To use this helm chart, first add the repository locally.
  helm repo add okapi https://indexdata.github.io/okapi-helm/
  Den Ingress erstmal auf false stellen.
  Okapi muss aber von außen erreichbar sein. Damit kommen die Überlegungen wie https, Ingress, Zertifikat ...
  Florian: Let's encrypt; sectigo
   Florian: kubectl Port forward; ein Kubernetes Service für Okapi
      https://kubernetes.io/docs/reference/kubectl/generated/kubectl_port-forward/#examples
