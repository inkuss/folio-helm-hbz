# 19.08.2024
# Testen
  helm template -f global-values.yaml zalando-pg
# Installieren
  helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
  # helm install --debug --dry-run -n folio-backend -f global-values.yaml zal-pg ./zalando-pg  -- ähnlich wie helm template, macht nichts, gibt das YAML aus
  kubectl describe -n folio-backend pods
# Löschen
  # helm delete -n folio-backend zal-pg
  helm uninstall -n folio-backend zal-pg

  Ein Secret liegt in einer privaten Container-Registry
  Anleitung: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  docker login
  A Kubernetes cluster uses the Secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.
  If you already ran docker login, you can copy that credential into Kubernetes:
  kubectl create secret generic regcred --from-file=.dockerconfigjson=$HOME/.docker/config.json --type=kubernetes.io/dockerconfigjson
    secret/regcred created
  To understand the contents of the regcred Secret you created, start by viewing the Secret in YAML format:
    kubectl get secret regcred --output=yaml
  To understand what is in the .dockerconfigjson field, convert the secret data to a readable format:
    kubectl get secret regcred --output="jsonpath={.data.\.dockerconfigjson}" | base64 --decode
  Das gleiche Secret habe ich auch noch einmal mit anderem Namen im Namensraum "folio-backend" erzeugt:
    kubectl get secret -n folio-backend folio-hbz-dockerhub-key --output="jsonpath={.data.\.dockerconfigjson}" | base64 --decode

  To pull the image from the private registry, Kubernetes needs credentials. 
  The imagePullSecrets field in the configuration file specifies that Kubernetes should get the credentials from a Secret named gitlab-folio-group .
  The image is: gitlab.bib-bvb.de:5050/folio/folio-tools:debian-v1.3
  In Job folio.init.job.yaml
  In turn, the image itself need environment variables from a secret key ref:
         env:
        - name: PGPASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres.folio-release-name.credentials.postgresql.acid.zalan.do
              key: password
   Weiterlesen hier https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/  "What's next"

  Für den Moment einfach nur postgresql auf das Cluster bringen, ohne die Secrets, User- und Init-Jobs/Scripts; so:
    kubectl -n folio-backend -f folio-zal-pg.yml apply
      Warning ...
      postgresql.acid.zalan.do/folio-zal-pg configured

22.08.2024
  https://kubernetes-io.translate.goog/docs/concepts/containers/images/?_x_tr_sl=auto&_x_tr_tl=de&_x_tr_hl=de#using-a-private-registry
    Lesen: Angeben von ImagePullSecrets auf einem Pod  (bekannt)
  aus folio-public/folio-tools ein Image bauen  ==> Das brauche ich nur, wenn ich nicht von bvb/folio-public herunter ziehen kann
    docker build https://gitlab.bib-bvb.de/folio-public/folio-tools
       Error response from daemon: dockerfile parse error line 5: unknown instruction: <!DOCTYPE
   anders: Repo klonen nach ~/folio-tools, dann
     cd ~/folio-tools
     docker build .
      Sending build context to Docker daemon  70.66kB
      Step 1/6 : FROM debian:bookworm-slim
      Get "https://registry-1.docker.io/v2/": dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:48605->[::1]:53: read: connection refused
---------------
   Erneuter Versuch zal-pg Deployment, nachdem Florian K. das images in der values.yml auf "folio-public" geändert hat
      helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
         error: INSTALLATION FAILED: failed post-install: timed out waiting for the condition
   create-pgsuperuser schlägt fehl (FAIL), Log ist dieses (das wird immer wieder versucht):
   - Waiting for file kube-scripts/create-pgsuperuser.sh...
    testing if kube-scripts/create-pgsuperuser.sh exists...done
    Thu Aug 22 17:43:12 UTC 2024 Creating SUPERUSER pgsuperuser ...
    psql: error: connection to server at "folio-zal-pg" (10.43.166.10), port 5432 failed: Connection refused
    ********************************************************************************************************
     Is the server running on that host and accepting TCP/IP connections?
         kubectl describe -n folio-backend pods
    Das Herunterziehen von folio-public geht jetzt !
   Beim Job "create-pgsuperuser" kommen jetzt folgende Events:
Events:
  Type    Reason     Age        From                  Message
  ----    ------     ----       ----                  -------
  Normal  Scheduled  <unknown>                        Successfully assigned folio-backend/create-pgsuperuser-snk6t to folio-k8s13
  Normal  Pulled     6m34s      kubelet, folio-k8s13  Container image "gitlab.bib-bvb.de:5050/folio-public/folio-tools:debian-v1.3" already present on machine
  Normal  Created    6m34s      kubelet, folio-k8s13  Created container create-pg-superuser
  Normal  Started    6m34s      kubelet, folio-k8s13  Started container create-pg-superuser
     Das passiert 6x (warum ?)
    - das Secret postgres.folio-zal-pg.credentials.postgresql.acid.zalan.do mit dem Passwort für den db-user postgres wurde angelegt.

      - "folio-zal-pg" steht auf Status: Pending und Grund: 0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.
                          - der Pod folio-zal-pg hat VolumeMounts und Volumes, aber sind die richtig definiert ?
                            die haben weder "name" noch "claimName"
      - aber auch das PVC "pgdata-folio-zal-pg" steht auf Status: Pending.
                -- der PVC sieht richtig aus, aber irgendwie gibt es kein Volume und keinen VolumeMount dazu. Siehe Cluster -> Persistent Volumes
                        die StorageClass local-path hat übrigens den Provisioner rancher.io
        das (folio-zal-pg und pgdate-folio-zal-pg) sind beides Services und ihre Manifestationen (YAML) sind im Dashboard unter Service - Services einsehbar.

23.08.
  in folio-zal-pg Pid
       volumeMounts:
        - mountPath: /home/postgres/pgdata
          name: pgdata
	  volumes:
    - name: pgdata
      persistentVolumeClaim:
        claimName: pgdata-folio-zal-pg-0
    - volumeName: pvc*   fehlt

    kind: postgres - Custom Resource Definition
     kubectl get crd

     nach Anpassen der CPUs / RAM, hat Pod folio-zal-pg jetzt den Status "Running" !
     Im Log kommen aber noch Fehler:
     Success. You can now start the database server using:
    /usr/lib/postgresql/13/bin/pg_ctl -D /home/postgres/pgdata/pgroot/data -l logfile start
2024-08-23 12:48:57.610 GMT [208] LOG:  invalid value for parameter "lc_monetary": "de_DE.UTF-8"
2024-08-23 12:48:57.610 GMT [208] LOG:  invalid value for parameter "lc_numeric": "de_DE.UTF-8"
2024-08-23 12:48:57.610 GMT [208] LOG:  invalid value for parameter "lc_time": "de_DE.UTF-8"
2024-08-23 14:48:57.610 CEST [208] FATAL:  configuration file "/home/postgres/pgdata/pgroot/data/postgresql.conf" contains er

      Nützliche Befehle:
       helm -n folio-backend list   - eine Liste, was in dem Namespace alles deployed ist.
       helm list - dasselbe für alle Namespaces
      mal das Log vom postgres-operator verfolgen:
       kubectl --namespace=default logs  "postgres-operator-8bc78d9f8-tzhhm"
      die verwendete Manifestation des Helm-Charts für zalando-pg, angereicherte Version:
       kubectl get -o yaml postgresqls.acid.zalan.do
       kubectl get postgresqls.acid.zalan.do   - Basisinformationen; was läuft
       kubectl get crd - listet Custom Resource Definitionen auf ("postgresql" von Zalando ist z.B. eine solche!)


      Florian: Jetzt noch einmal starten:
 helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
    - bei Job create-pgsuperuser kommt wieder "connection refused"
    Error: INSTALLATION FAILED: failed post-install: 1 error occurred:
        * job create-pgsuperuser failed: BackoffLimitExceeded
   -> evtl. noch einmal alles löschen und die "lc_monetary" ... Fehler in postgresql.conf beheben

26.08.2024
  UTF-8 => utf8 in den Parametern lc_monetary etc. (in values.yaml)
  -- Waiting for file kube-scripts/create-pgsuperuser.sh...
  testing if kube-scripts/create-pgsuperuser.sh exists...done
  Mon Aug 26 10:57:52 UTC 2024 Creating SUPERUSER pgsuperuser ...
  psql: error: connection to server at "folio-zal-pg" (10.43.21.168), port 5432 failed: Connection refused
   Is the server running on that host and accepting TCP/IP connections?

   Jetzt die Parameter entfernt (lc_monetary, ...)
   - in Datenbanklog kommen CREATE und EXEC Anweisungen (gut)
  2024-08-26 11:10:34,381 WARNING: Could not activate Linux watchdog device: Can't open watchdog device: [Errno 2] No such file or directory: '/dev/watchdog'
  - der Job create-pgsuperuser schlägt weiterhin fehl (Verbindung zur Datenbank kann nicht hergestellt werden)
  - das persistente Volume für die pg-Datenbank wurde definitiv erfolgreich zur Verfügung gestellt. Die Datenbank wurde hier angelegt:
    root@folio-k8s13:/var/lib/rancher/k3s/storage/pvc-d016e739-d9db-4634-9182-13b98c9eb19c_folio-backend_pgdata-folio-zal-pg-0/pgroot/
  Jetzt kommen Warnungen im Postgres Log: 
  2024-08-26 11:10:36,057 WARNING: Kubernetes RBAC doesn't allow GET access to the 'kubernetes' endpoint in the 'default' namespace. Disabling 'bypass_api_service'.
  2024-08-26 11:10:36,070 WARNING: postgresql parameter listen_addresses=* failed validation, defaulting to None
  2024-08-26 11:10:36,071 WARNING: postgresql parameter wal_keep_segments=0 failed validation, defaulting to 8
  2024-08-26 11:10:36,071 WARNING: postgresql parameter wal_log_hints=off failed validation, defaulting to on

  "uninstall" anstatt "delete" verwenden:
    helm uninstall -n folio-backend zal-pg
    release "zal-pg" uninstalled
Job zal-pg-folio-init-job geht jetzt:
  -- Waiting for file kube-scripts/folio-db-init.sh...
testing if kube-scripts/folio-db-init.sh exists...done
Mon Aug 26 11:44:55 UTC 2024 Creating databases okapi and folio ...
Mon Aug 26 11:44:55 UTC 2024 Running: psql -w -v ON_ERROR_STOP=1 -h folio-zal-pg -p 5432 -U postgres ...
CREATE ROLE
CREATE DATABASE
CREATE ROLE
CREATE DATABASE
Mon Aug 26 11:44:55 UTC 2024 Init script is completed

Job create-pgsuperuser war jetzt auch erfolgreich:
-- Waiting for file kube-scripts/create-pgsuperuser.sh...
testing if kube-scripts/create-pgsuperuser.sh exists...done
Mon Aug 26 11:44:52 UTC 2024 Creating SUPERUSER pgsuperuser ...
CREATE ROLE
Mon Aug 26 11:44:52 UTC 2024 User created!

Jetzt war erfolgreich:
helm install -n folio-backend -f global-values.yaml zal-pg ./zalando-pg
NAME: zal-pg
LAST DEPLOYED: Mon Aug 26 13:44:51 2024
NAMESPACE: folio-backend
STATUS: deployed
REVISION: 1
TEST SUITE: None

Jetzt läuft erfolgeich: Pods folio-zal-pg-0, folio-zal-pg-1 im Namespace folio-backend; mit den 4 Warnungen wie oben genannt.
Weiter mit Installation Okapi.

30.08.2024
Versuche, Okapi zu installieren
Versuche, offizielles Image von folio-org zu ziehen, dieses hier: https://hub.docker.com/r/folioci/okapi
  cd ~/folio-helm-hbz
  helm install -n folio-backend -f global-values.yaml okapi ./okapi
  helm uninstall -n folio-backend okapi
 Normal   Pulled     34s                kubelet            Successfully pulled image "folioorg/okapi:latest" in 815ms (815ms including waiting). Image size: 100319551 bytes.
  Warning  BackOff    6s (x6 over 79s)   kubelet            Back-off restarting failed container okapi in pod okapi-5f79959657-z6tl2_folio-backend(d0e8fd4c-cf0d-49d6-85e7-5ecc7b3d9005)
  Florian: Besser  Helm-Chart von IndexData verwenden.
 Uwe 
   podman pull gitlab.bib-bvb.de:5050/folio-public/okapi:v5.2.0
   Trying to pull gitlab.bib-bvb.de:5050/folio-public/okapi:v5.2.0...
   Error: initializing source docker://gitlab.bib-bvb.de:5050/folio-public/okapi:v5.2.0: Requesting bearer token: invalid status code from registry 403 (Forbidden)
   
Florian
https://github.com/indexdata/okapi-helm/tree/main
  To use this helm chart, first add the repository locally.
  helm repo add okapi https://indexdata.github.io/okapi-helm/
  Den Ingress erstmal auf false stellen.
  Okapi muss aber von außen erreichbar sein. Damit kommen die Überlegungen wie https, Ingress, Zertifikat ...
  Florian: Let's encrypt; sectigo
   Florian: kubectl Port forward; ein Kubernetes Service für Okapi
      https://kubernetes.io/docs/reference/kubectl/generated/kubectl_port-forward/#examples

09.09.2024
  cd ~/folio-helm-hbz
  git clone https://github.com/indexdata/okapi-helm.git
  cd okapi-helm
  edit okapi/values.yaml
  helm uninstall -n folio-backend okapi
  helm install -n folio-backend -f ../global-values.yaml okapi ./okapi

  1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace folio-backend -l "app.kubernetes.io/name=okapi,app.kubernetes.io/instance=okapi" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace folio-backend $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo $POD_NAME
okapi-6bc7d758cd-hvtrw
  echo $CONTAINER_PORT
9130
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace folio-backend port-forward $POD_NAME 8080:$CONTAINER_PORT
Forwarding from 127.0.0.1:8080 -> 9130
Forwarding from [::1]:8080 -> 9130

  kubectl describe -n folio-backend pods
  ...
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  6m9s                  default-scheduler  Successfully assigned folio-backend/okapi-6bc7d758cd-hvtrw to folio-k8s12
  Normal   Pulled     6m6s                  kubelet            Successfully pulled image "folioorg/okapi:5.1.2" in 3.652s (3.652s including waiting). Image size: 104746777 bytes.
  Normal   Pulled     5m56s                 kubelet            Successfully pulled image "folioorg/okapi:5.1.2" in 805ms (805ms including waiting). Image size: 104746777 bytes.
  Normal   Pulled     5m26s                 kubelet            Successfully pulled image "folioorg/okapi:5.1.2" in 807ms (807ms including waiting). Image size: 104746777 bytes.
  Normal   Created    4m56s (x4 over 6m6s)  kubelet            Created container okapi
  Normal   Started    4m56s (x4 over 6m6s)  kubelet            Started container okapi
  Normal   Pulled     4m56s                 kubelet            Successfully pulled image "folioorg/okapi:5.1.2" in 911ms (911ms including waiting). Image size: 104746777 bytes.
  Normal   Pulling    4m1s (x5 over 6m10s)  kubelet            Pulling image "folioorg/okapi:5.1.2"
  Warning  BackOff    67s (x22 over 5m48s)  kubelet            Back-off restarting failed container okapi in pod okapi-6bc7d758cd-hvtrw_folio-backend(be154412-cf25-4215-82ca-dab6fec04890)

  und im Log vom Pod okapi auf folio-k8s12:
2024-09-09T13:52:47,357 ERROR ? FATAL: password authentication failed for user "okapi" (28P01) io.vertx.pgclient.PgException: FATAL: password authentication failed for user "okapi" (28P01) | 
2024-09-09T13:52:47,358 INFO ? shutdown 

  In Dashboard - folio-backend - Services - okapi
Endpoints 
Host 		Ports (Name, Port, Protokoll) 		Node 		Bereit
10.42.1.39 	okapi,9130,TCP hazelcast,5701,TCP 	folio-k8s12
-
  Versuche, Okapi anzusprechen:
  curl -XGET http://folio-k8s12.hbz-nrw.de:9130/_/env
curl: (7) Failed to connect to folio-k8s12.hbz-nrw.de port 9130 after 1 ms: Couldn't connect to server
  Das kann aber an "password authentication failure" liegen.

Einen neuen Job und Skript create-pg-okapiuser eingerichtet. => nein, das ist alles schon im Job folio.db.init  geschehen.
Ich muss nur das richtige Okapi-Passwort verwenden.
Das Okapi-Passwort steht z.Zt. in values.yaml in folio-helm-hbz/okapi-helm/okapi secret drin.

Es kommt aber weiterhin
  2024-09-09T16:14:33,188 ERROR ? FATAL: password authentication failed for user "okapi" (28P01) io.vertx.pgclient.PgException: FATAL: password authentication failed for user "okapi" (28P01) | 

Wie wird das Passwort in dem IndexData Helm Chart gezogen, wie verbindet sich Okapi mit der Datenbank ?
- es gibt eine secret.yaml, die baut aus Values.secret eine okapi-config.json
- in deployment.yaml wird dann aus dem Secret ein Volume gebaut und dieses im Okapi-Container gemountet.
  => die Okapi-config.json anders bauen, nicht aus Values.secret sondern aus dem schon vorhandenen Secret folio-db-secret.
     Dann die anderen Parameter zu der okapi-config.json hinzufügen; nur das $OKAPI_PASSWORD ist geheim.
     secret.yaml baut einfach ein Secret "okapi" (=okapi.fullname) mit dem Inhalt StringData (=der Inhalt der okapi-config.json).

11.09.2024
  Quelle: https://stackoverflow.com/questions/59082646/use-kubernetes-secrets-as-environment-variables-inside-a-config-map
  die config map ist statisch, ich kann in sie keine Platzhalter herein schreiben
  Ich muss einen Job anlegen, der ein sh-Skript ausführt, welches die config-map schreibt.

   Achtung! YAMLs in templates.HIST/ werden auch ausgeführt, diese löschen !!
   Secret folio-db-secret geändert, dieses muss explizit gelöscht werden; das über das Dashboard machen.
   Das Secret wird nicht wieder angelegt, in der Folge wird folio.init.job nicht fertig :-(
     dann lege ich das Secret eben manuell wieder an: kubectl -n folio-backend -f /tmp/folio-db-secret.yaml apply
   Jetzt ist Okapi erfolgreich hoch gekommen:
  Normal  Started    2m    kubelet            Started container okapi
  Im log vom Pod Okapi:
  2024-09-11T13:30:54,990 INFO ? 592314/proxy REQ 10.42.1.1:46856 supertenant GET /_/proxy/health okapi-5.1.2 
  2024-09-11T13:30:54,990 INFO ? 913644/proxy RES 200 1047us okapi-5.1.2 /_/proxy/health 

   Als nächstes das mit dem Ingress ausprobieren;
   z.Zt. ist ingress: enabled: false !
   - wie geht das mit dem kubectl port-forward ?

13.09. 
Florian G.: Ich brauche eine Ingress - Klasse
  ich brauche Ingress-Controller, ein laufender nginx-Pod, der guckt, gibt es Ingress-Objekte von meiner Klasse.
   der Proxy-Server von außen muss auf die IP-Adressen von Kubernetes zugreifen;
      mit Ingress wird es auf k8s-Seite einfacher; Ingress-Controller installieren. Load-Balancer installieren
             (für Sachen von außen)
	     https://kubernetes.github.io/ingress-nginx
   in dem Netz, in dem die virtuellen Maschinen sind, muss ich mehrere IP-Adressen reserviert bekommen.
     Diese kann dann der Load-Balancer genutzen. Dann habe ich eine Load-Balancer IP-Adresse.
     Auf diese IP-Adresse setzt dann der Ingress-Controller auf.
       IP-Adresse kann sich ändern, Service-Name nicht.
       Der interen DNS-Server von K8s wird auch über IP-Adresse angepsrochen.

       Denis: den internern Diensten einen node-Port verpassen.
       Okapi an 3 Nodes...

16.09.
  Installation des nginx-Ingress-Controller
  helm upgrade --install ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx --namespace folio-backend
  kubectl get service ingress-nginx-controller --namespace=folio-backend
  NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP                     PORT(S)                      AGE
  ingress-nginx-controller   LoadBalancer   10.43.213.243   10.9.2.91,10.9.2.92,10.9.2.93   80:32702/TCP,443:31664/TCP   3m1s

19.09.2024
  hbz gitlab folio Projekt klonen:
  cd folio/folio-helm-hbz
  git clone http://quaoar4.hbz-nrw.de:8929/bms/folio.git folio-hbz
  # Geheime Informationen in quaoar-Repo speichern und nur symbolisch verknüpfen:
  ln -s folio-hbz/global-values.yaml
  ln -s folio-hbz/folio-hbz-dockerhub-key.yml

20.09.2024 d-sys-ops
  Okapi mit Service Type "NodePort" installieren
     cd okapi-helm  (Indexdata Helm Chart)
     vim okapi/values.yaml
       chaned one line:
        service:
   enabled: true
   name: okapi
-  type: ClusterIP
+  type: NodePort

     vim okapi/templates/service.yaml
       removed three lines:
        spec:
-  clusterIP: None
   type: {{ .Values.service.type }}
   ports:
     - port: {{ .Values.service.port }}
       name: okapi
-      nodePort: 9130
     - port: {{ .Values.service.second_port.port }}
       name: hazelcast
   selector:
     {{- include "okapi.selectorLabels" . | nindent 4 }}
-  type: NodePort

     helm upgrade -n folio-backend  okapi ./okapi
     kubectl get services -n folio-backend
     NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP                     PORT(S)
     ...
okapi                                NodePort       10.43.126.141   <none>                          9130:30935/TCP,5701:32127/TCP   148m
    jetzt geht: curl -XGET folio-k8s11:30935/_/env, mit einem beliebigen Servernamen der Server im Cluster
                                                    (auch einem Server, auf dem Okapi gar nicht läuft!)
    Der Port 30935 bleibt angeblich gleich, zumindest solange man Okapi nur mit "helm upgrade" verändert.

  wg. okapi-config.json, Datenbank-Passwort für den Okapi-Benutzer. Aus values-Datei vs. aus Secret:
    Florian nutzt keine okapi-config.json, sondern übergibt alle Okapi-Variablen als ENV-Vars !
    Dann kann er das Secret folio-db-secret auch für Okapi benutzen.
    Wie das geht: siehe meine Screenshots von heute:
      cd okapi/templates
      vim deployment.yaml:
      env
      -  name: PG_PASSWORD
         valueFrom:
	   secretKeyRef:
	     name: {{ .Values.global.db.folioSecret }} ==> folio-db-secret
	     key: {{ .Values.global.db.okapiPwKKey }}  ==> OKAPI_PASSOWRD
	deployment.yaml entsprechend angepasst.
    Das Secret okapi-config benötigt er nicht (davon ab, dass er sowieso kein Indexdata Helm Chart benutzt).
      => das in zalando_pg/templates/folio.secret.yaml wieder herausgeworfen.
    Und auch das Array in der .Values.secret wird nicht mehr benötigt; bei mir wurde das allerdings auch schon
    herausgeworfen und durch einen Eintrag in der folio-hbz/global-values.yaml ersetzt: dort auch herauswerfen. => OK, okapiPassword: okapi25 dort heraus geworfen.
    Alle anderen Parameter außer OKAPI_PASSWORD versuche ich weiterhin über eine okapi.conf zu laden => das Array .Values.secret wieder aktiviert, allerdings ohne postrges_password.
     cd okapi-helm
     helm upgrade -n folio-backend  -f ../global-values.yaml okapi ./okapi
     Release "okapi" has been upgraded. Happy Helming!
      NAME: okapi
      LAST DEPLOYED: Fri Sep 20 21:39:42 2024
      NAMESPACE: folio-backend
      STATUS: deployed
      REVISION: 6
    ok, hat funktioniert. Den okapi Ingress noch wieder entfernen (in values.yaml auf false setzen).
    curl -XGET folio-k8s11:30935/_/env []
    curl -XGET folio-k8s11:30935/_/proxy/health
      [ ]

    als nächstes Kafka und Elasticsearch als bitnami Helm Chart installieren, s. BVB-Repo / erl.

  nützliche Tipps / git:
  alle git-Commits zu einer bestimmten Datei anzeigen:
    git log -- values.yamlgit log -- values.yaml
  eine bestimmte git-Version einer Datei anzeigen: 
    wgit show dd2f3d9f124700a0a0149d00f43bb6a18f5c1773 -- values.yaml

  psql in einem pg-Container aufrufen; Dazu kann man sich einfach wie bei docker in den Container einloggen:
    kubectl -n folio-backend exec -it folio-zal-pg-0 -- bash
   oder nicht-interaktiv z.B. so:
    kubectl -n folio-backend exec folio-zal-pg-0 -- nslookup okapi
   In dem Container sind dann die Umgebungsvariablen bekannt, z.B. OKAPI_URL, SUPERUSER_PASSWD

   [ Diksussion über den Kubernetes Operator;
     - Wissen im Operator SDK ist nur während der Entwicklungsarbeiten notwendig, bei der Anwendung des Operators nicht mehr; https://sdk.operatorframework.io/build/
     - Kubernetes Operator wird die Module auch deployen
     - Upgrades mitdenken
     - Migration ist out-of-scope, da DB-Deployment auch nicht dazu gehört
   ]
   Ghassen: End-of-Live von Okapi ? 
     - zunächst ist es im Kubernetes Operator noch drin
     - auch mgr-application hat ein Feld OKAPI_URL (wird es genutzt ?)
     - Okapi-Support wird nicht vor Beginn des U-Releases enden.

   Ghassen: wer deployet die Module im Eureka-Context ?
   Florian: folio-org/mgr-applications ist dafür angedacht, aber es steht dort "optional". EBSCO macht es nicht damit.

    eureka-platform-boot : Eureka mit docker-compose klonen und das Skripgt start-docker-containers.sh ausprobieren,
      dann an den Fehlermeldungen weiter hangeln.
      git clone https://github.com/folio-org/eureka-platform-bootstrap.git
      Julian: Diese Plattform (Eureka ohne Kubernetes) wurde entwickelt, weil die AWS-Kosten für das Hosting
         (der Development-Plattform(en)) "unter die Decke gingen".

23.09.2023
  Kafka im Namensraum folio-staging-v1-kafka erneut installiert; mit Bitnami Helm Chart;
  Values-Datei kafka-hbz-staging-values.yml angepasst,
     siehe alles in  https://github.com/inkuss/kube.git .
  Installation Elasticsearch ebenfalls in diesem Repo beschrieben.
   
  weiter mit: Create a new tenant; dann: Install a FOLIO backend.
